TO avoid crawling the same page twice, it is extremely important taht all internal links discovered are formatted consistenly , and kept in a running set for easy lookups, while the program is running.

Before you start writing a crawler that follows all outbounds links, you should ask yourself a few questions:

1. What data am I trying to gather?

2. When my crawler reaches a particular website, will it immediately folllow the next outbound link to a new website, or will it stick around for a while and drill down into the current website?

3. Are there any conditions under which I would not want to scrape a particular site?

4. How am I protecting myself against legal action if my web crawler catches the attention of a webmaster on one of the sites it runs across?


Planning and Defining Objects
--

What do I need?

Will this information help with the project goals? Will it be a roadblock if I dont have it, or is it just "nice to have" but won't ultimately impact anything?

If it might help in the future, but I'm unsure, how difficult it will be to go back and collect the data at a later time?

Is this data redundant to data I've already collected?

Does it make logical sense to store the data within this particular object? 

---

Is this data sparse or dense? Will it be relevant and populated in every listing or just a handful out of the set?

How large is the data?

Especially in the case of large data, will I need to regualrly retrieve it every time I run my analysis, or only on accasion?

How variable is this type of data? Will I regularly need to add new attributes, modify types, or is it  set in stone?

---

As you start to add scraper functions for additional new sites, ypou might notice a pattern forming. Every site's parsing function does essentially the same thing:

- Selects the title element and extracts the text for the title
- Selects the main content of the article
- Selects other content items as needed
- Return a COntent object instantiated with the strings found previously


SCRAPY
---

Install
Initialize a new spider
Writing a simple scraper
Spidering with rules
Creating Items
Outputting Items
The Item Pipeline
Logging with Scrapy


Storing Data
---


Bradth-first search
--

The mainfunction, searcgBreath, works recursively to construct a list of all possible paths from the search page and stops when it finds a path that has reached the target path:

- It starts with a single path, [1], representing a path in which the user stays on the target page with the ID 1 and follows no links.

- For eah path in the list of paths, it gets all of the links that link out from the page represented by the last page in the path.

- For each of these outbound links, it checks wheteher tehy match the targetPageId. If there's a match, taht path is returned.

- If tehre's no match, a new path is added to a new list of (now longer) paths, consisting of the old path + the new outbound page link.

- If the targetPage Id is not found at this level at all, a recursion occurs and searchBreadth is called with the same targetPageId and new, longer, list of paths.


After the list of page IDs containing a path between the two pages found, each ID is resolved to its actual URL and printed.
